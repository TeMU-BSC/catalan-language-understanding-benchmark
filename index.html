<!DOCTYPE html><!--Author: BSC - TeMU 2022--><html><head><meta charset="utf-8"><title>The Spanish Evaluation Benchmark</title><meta name="description" content="The Spaish Evaluation Benchmark (EvalES) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="./logo.png"><link rel="image_src" type="image/png" href="./logo.png"><link rel="shortcut icon" href="./favicon.ico" type="image/x-icon"><link rel="icon" href="./favicon.ico" type="image/x-icon"><link rel="stylesheet" href="./bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="./stylesheets/layout.css"><script src="./javascripts/analytics.js"></script><script src="./bower_components/jquery/dist/jquery.min.js"></script><script src="./bower_components/bootstrap/dist/js/bootstrap.min.js"></script><link rel="stylesheet" href="./stylesheets/index.css"><script async defer src="https://buttons.github.io/buttons.js"></script><script src="./javascripts/mi_script.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="./">Home</a></li><li><a href="./datasets.html">Datasets</a></li><li><a href="./submit.html">Submit</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="./">EvalES</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle">Eval<b>ES</b></h1><h2 id="appSubtitle">The Spanish Evaluation Benchmark</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-15"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>Leaderboard</h2></div><p class="spaced">SQuAD2.0 tests the ability of a system to not only answer reading comprehension questions, but also abstain when presented with a question that cannot be answered based on the provided paragraph.</p><div id="leaderboard"></div></div></div></div><div class="col-md-15"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>What is EvalES?</h2></div><p> <span>Eval<b>ES </b></span>is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or <i>span</i>, from the corresponding reading passage, or the question might be unanswerable.</p><hr><p>Eval<b> ES </b> combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.</p><a class="btn actionBtn" href="/explore/v2.0/dev/">Explore SQuAD2.0 and model predictions</a><a class="btn actionBtn" href="http://arxiv.org/abs/1806.03822">SQuAD2.0 paper (Rajpurkar & Jia et al. '18)</a><h2>Have Questions?</h2><p> Ask us questions at    <a href="mailto:plantl-gob-es@bsc.es">plantl-gob-es@bsc.es</a>.</p></div><div class="infoSubheadline"><a href="https://twitter.com/share" class="twitter-share-button" data-url="https://temu.bsc.es" data-text="The spanish language comprehension test" data-via="TeMU_BSC" data-size="large" data-hashtags="PlanTL">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script><!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/TeMU-BSC/spanish-benchmark" data-icon="octicon-star" data-size="large" aria-label="Star TeMU-BSC/spanish-benchmark on GitHub">Star</a></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="centerNav"><div><img src="./gob-es.png"><img src="./secretaria-es.png"></div></div></div></nav></body></html>