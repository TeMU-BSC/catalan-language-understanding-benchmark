<!DOCTYPE html><!--Author: BSC - TeMU 2022--><html><head><meta charset="utf-8"><title>Datasets - CLUB</title><meta name="description" content="CLUB Dataset sources and information"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="./logo.png"><link rel="image_src" type="image/png" href="./AINA_header.png"><link rel="shortcut icon" href="./favicon-16x16.png" type="image/png"><link rel="icon" href="./favicon-16x16.png" type="image/png"><link rel="stylesheet" href="./bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="./stylesheets/layout.css"><script src="./bower_components/jquery/dist/jquery.min.js"></script><script src="./bower_components/bootstrap/dist/js/bootstrap.min.js"></script><link rel="stylesheet" href="./stylesheets/index.css"></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="./">Home</a></li><li><a href="./datasets.html">Datasets</a></li><li><a href="./submit.html">Submit</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="./">CLUB</a></div></div></div></div><div class="cover" id="topCover"><!-- img#titleImg(src="./índex.png")--><div class="container"><div class="row"><div class="col-md-12"><!-- h1#appTitle CLUB--><!-- h2#appSubtitle The Catalan Language Understanding Bencmark--></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-15"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"></div><h2>Datasets</h2><div class="datasetHolder" id="data"><h3>Data statements</h3><p>In general, we provide as much curation information as possible following (Bender and Friedman, 2018), when relevant. For example, gender and socioeconomic status are considered not as relevant for the kind of semantic annotations created. However, the fact that all commissioned annotators (1) were native speakers of Catalan, (2) were translators, editors, philologists or linguists, and (3) had previous experience in language-related tasks, is considered to be important. The curation rationale we follow was to make these datasets both representative of contemporary Catalan language use, as well as directly comparable to similar reference datasets from the General Language Understanding Evaluation (GLUE) benchmark. Since our datasets are geared towards Machine Learning and Language Modelling, we have provided training, evaluation and tests splits at HuggingFace. In what follows, we describe some of these datasets.</p></div><div class="datasetHolder" id="sts"><h3>Semantic Textual Similarity: STS-ca</h3><p>For Semantic Textual Similarity (STS), we create a new dataset from scratch, STS-ca. It contains more than 3000 sentence pairs, annotated with their semantic similarity using a scale from 0 (no similarity at all) to 5 (semantic equivalence).
To develop STS-ca, we pre-selected potential sentence pairs from the CaText corpus by using different similarity measures (Jaccard, Doc2Vec and DistilBERT embedding cosine similarity). We did a final manual review to ensure that the selection represented superficial and deep similarities in subject matter and lexicon. Following the guidelines set in the SemEval challenges, we commissioned 4 native speaker annotators from 2 independent companies to assess the similarity of the sentence pairs on a scale between 0 (completely dissimilar) to 5 (completely equivalent), with other possible values, such as 3 (roughly equivalent, but some important information differs). Then, for each sentence pair, we computed the mean of the four annotations, and we discarded single annotations that deviate by more than 1 from the mean. After this filtering process, we used the mean of the remaining annotations as a final score. Finally, in order to assess the quality of the dataset, we measured the correlation of each annotator’s labels with the average of the rest of the annotators, and averaged all the individual correlations, resulting in a Pearson correlation of 0.739.</p></div><div class="datasetHolder" id="ner"><h3>POS and NERC from AnCora</h3><p>For Part-of-Speech Tagging (POS) and Named Entity Recognition and Classification (NERC) evaluations, we use annotations from the of the wellknown AnCora corpus, projected on the Universal Dependencies treebank. We extracted Named Entities from the original AnCora version, filtering out some unconventional ones, like book titles, and transcribe them into a standard CONLL-IOB format. The AnCora corpus has been released recently under CC-BY licence, and we published our derivative version under the same licence.</p></div><div class="datasetHolder" id="vilaquad"><h3>VilaQuAD</h3><p>Following the same guidelines as ViquiQuAD, we developed VilaQuAD, an extractive QA dataset from newswire. From a the online edition of the catalan newspaper Vilaweb, 2095 article headlines were randomnly selected. These headlines were also used to create the Textual Entailment dataset (TECa). For the extractive QA dataset, creation of between 1 and 5 questions for each news context was commissioned to a team of native speakers. In total, 6282 pairs of a question and an extracted fragment that contains the answer were created.</p></div><div class="datasetHolder" id="viquiquad"><h3>ViquiQuAD</h3><p>Is an extractive Question Answering dataset using content from the Catalan Wikipedia (Viquièpdia). From a set of high quality, non-translation, articles in the Catalan Wikipedia, 597 were randomly chosen, and from them 5-8 sentence contexts were extracted. We commissioned the creation of between 1 and 5 questions for each context, following an adaptation of the guidelines from SQUAD 1.0. Annotators were native language speakers. In total, 15153 pairs of a question and an extracted fragment that contained the answer were created.</p></div><div class="datasetHolder" id="xquad"><h3>XQuAD-Ca</h3><p>The Cross-lingual Question Answering Dataset is a multilingual benchmark for evaluating question answering performance. The dataset consists of a subset of 240 paragraphs from the Wikipedia and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Rumanian was added later. We created a Catalan version using professional translators.</p></div><div class="datasetHolder" id="teca"><h3>TECa: Textual Entailment for Catalan</h3><p>Textual entailment (TE) is the directional relation between two sentences, a premise and an hypothesis. For every premise the relation with an hypothesis can be Neutral, Inference or Contradiction. Textual Entailment is considered an important evaluation of the ability of a model to incorporate some measure of inferential capabilities, as opposed to make do with a prodigious memory of what it has actually seen before. TECa contains more than 20000 pairs of sentences annotated with their relation label, that can be 0 (neutral), 1 (inference) or 2 (contradiction).
We randomly chose 18000 sentences from those sources, and filtered them by different criteria, such as length and stand-alone intelligibility. From the remaining text sentences, we commissioned 3 hypotheses (one for each entailment category) to be written by a team of annotators. We obtained more than 20000 annotated sentence pairs, which are published under CC-by licence. From 600 randomly selected samples we cross-annotated for Quality Assurance, and obtained an inter-annotator agreement of 83,57%.</p></div><div class="datasetHolder" id="tecla"><h3>TeCla: Text Classification</h3><p>TeCla (Textual Classification for Catalan) is a Catalan News corpus for thematic Text Classification tasks. It contains 153265 articles classified under 30 different categories, albeit editorially-oriented ones rather than truly encyclopedic labels.
We crawled 219586 articles from the Catalan News Agency (ACN) newswire archive, the latest from October 11, 2020. We used the subsection category asa classification label, after excluding territorial labels and labels with less than 2000 occurrences. With this criteria we compiled a total of 153265 articles for this text classification dataset.}</p></div></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="centerNav"><div><img src="./secretaria-es.png"></div><div><img src="./gob-es.png"></div></div></div></nav></body></html>