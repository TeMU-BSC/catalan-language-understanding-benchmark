<!DOCTYPE html><!--Author: BSC - TeMU 2022--><html><head><meta charset="utf-8"><title>Datasets - The Spanish Evaluation Benchmark</title><meta name="description" content="The Spaish Evaluation Benchmark (EvalES) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="./logo.png"><link rel="image_src" type="image/png" href="./logo.png"><link rel="shortcut icon" href="./favicon.ico" type="image/x-icon"><link rel="icon" href="./favicon.ico" type="image/x-icon"><link rel="stylesheet" href="./bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="./stylesheets/layout.css"><link rel="stylesheet" href="./stylesheets/index.css"><script src="./javascripts/analytics.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="./">Home</a></li><li><a href="./datasets.html">Datasets</a></li><li><a href="./submit.html">Submit</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="./">EvalES</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle">Eval<b>ES</b></h1><h2 id="appSubtitle">The Spanish Evaluation Benchmark</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-15"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"></div><h2>Datasets</h2><div class="datasetHolder"><p><b>Text classification</b></p><p>The Multilingual Document Classification Corpus (MLDoc) (Schwenk and Li, 2018; Lewis et al., 2004) is a cross-lingual document classification dataset covering 8 languages. We used the Spanish portion to evaluate our models on monolingual classification. It consists of 14,458 news articles from Reuters classified in four categories: Corporate/Industrial, Economics, Government/Social and Markets.</p></div><div class="datasetHolder"><p><b>Named Entity Recognition and Classification (NERC)</b></p><p>We selected the CoNLL-NERC and the CAPITEL-NERC datasets. CoNLL-NERC is the Spanish dataset of the CoNLL-2002 Shared Task (Tjong Kim Sang, 2002). The dataset is annotated with four types of named entities: persons, locations, organizations, and other miscellaneous entities. They are formatted in the standard Beginning-Inside-Outside (BIO) format. The dataset is composed of 8,324 sentences with 19,400 named entities for the training set, 1,916 sen- tences with 4,568 named entities for the development set, and 1,518 sentences with 3,644 named entities for the test set. CAPITEL-NERC was the first sub-task of the CAPITEL-EVAL shared task, held by IberLEF in 2020. The source of the CAPITEL-NERC datasets is the CAPITEL corpus12 (Porta-Zamorano and Espinosa-Anke, 2020), a collection of Spanish articles in the news domain. The dataset consists of 22,647 sentences with 31,311 named entities for train, and 7,550 sentences for development and test sets respectively, with 10,229named entities for the development set and 10,226 for the test set. CAPITEL-NERC is annotated with the same four named entities used in CoNLL-NERC (persons, locations, organizations, and other), but following a Beginning-Inside-Outside-Ending-Single (BIOES) format.</p></div><div class="datasetHolder"><p><b>Paraphrase Identification</b></p><p>The Crosslingual Adversarial Dataset for Paraphrase Identification (PAWS-X) (Yang et al., 2019) is a multilingual dataset that contains 49,401 training sentences, 2,000 sentences for the development set, and another 2,000 for the test set. It is important to note that this dataset contains machine translated text, and as a consequence some of the Spanish sentences might not be entirely correct.</p></div><div class="datasetHolder"><p><b>Part-of-Speech Tagging (POS)</b></p><p>We selected the Universal Dependencies Part-of-Speech (UD-POS) dataset, from the Spanish Ancora corpus13 (Taulé, Martí, and Recasens, 2008), and the CAPITEL-POS from the CAPITEL Corpus, described above.</p></div><div class="datasetHolder"><p><b>Semantic Textual Similarity (Agirre et al., 2012)</b></p><p>We collected the Spanish test sets from 2014 (Agirre et al., 2014) and 2015 (Agirre et al., 2015). Since no training data was provided for the Spanish subtask, we randomly sampled both datasets into 1,321 sentences for the train set, 78 sentences for the development set, and 156 sentences for the test set. To make the task harder for the models, we purposely made the development set smaller than the test set.</p></div><div class="datasetHolder"><p><b>Textual Entailment</b></p><p>We used the Spanish part of the Cross-Lingual NLI Corpus (XNLI) (Conneau et al., 2018). This evaluation corpus consists of a collection 400,202 sentences, annotated with textual entailment via crowdsourcing.</p></div><div class="datasetHolder"><p><b>Question Answering (QA)</b></p><p>We built a new dataset, the Spanish Question Answering Corpus (SQAC), an extractive QA dataset that we exhaustively present in section 3.2.1. There is no sizable training dataset analogous to the English version of SQUAD (Rajpurkar et al., 2016), and most finetunings of Spanish models rely on machine translated text. There is a professionally translated version of the XQUAD (Artetxe, Ruder, and Yogatama, 2019) dataset, but it is not bigenough or varied enough to properly train or evaluate, and the source text is not written originally in Spanish (and translation artifacts could slip in).</p></div></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="centerNav"><div><img src="./gob-es.png"><img src="./secretaria-es.png"></div></div></div></nav><script src="./bower_components/jquery/dist/jquery.min.js"></script><script src="./bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>