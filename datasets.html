<!DOCTYPE html><!--Author: BSC - TeMU 2022--><html><head><meta charset="utf-8"><title>Datasets - CLUB</title><meta name="description" content="CLUB Dataset sources and information"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="./logo.png"><link rel="image_src" type="image/png" href="./AINA_header.png"><link rel="shortcut icon" href="./favicon-16x16.png" type="image/png"><link rel="icon" href="./favicon-16x16.png" type="image/png"><link rel="stylesheet" href="./bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="./stylesheets/layout.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20,400,0,0"><script src="./bower_components/jquery/dist/jquery.min.js"></script><script src="./bower_components/bootstrap/dist/js/bootstrap.min.js"></script><link rel="stylesheet" href="./stylesheets/index.css"><script src="./javascripts/navbar-prevent-on-html-link.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="./">Home</a></li><li><a href="./datasets.html">Datasets</a></li><li><a href="./submit.html">Submit</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="./">CLUB</a></div></div></div></div><div class="cover" id="topCover"><!-- img#titleImg(src="./índex.png")--><div class="container"><div class="row"><div class="col-md-12"><!-- h1#appTitle CLUB--><!-- h2#appSubtitle The Catalan Language Understanding Bencmark--></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-15"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"></div><h2>Datasets</h2><div class="datasetHolder" id="data_statement"><h3>Data statement</h3><p>Following (Bender and Friedman, 2018) we provide all the relevant information about the datasets of this benchmark. All the commissioned annotators of these datasets (1) were native speakers of Catalan, (2) with a background on translation, edition, philology, or linguistics, and (3) had previous experience in language-related tasks. The curation rationale we followed was to make these datasets both representative of contemporary Catalan language, as well as directly comparable to similar reference datasets from the General Language Understanding Evaluation (GLUE) benchmark. Since our datasets are desgined for Machine Learning and Language Modelling, we have provided training, evaluation and tests splits at HuggingFace. In what follows, we describe these datasets.</p></div><div class="datasetHolder" id="ner"><h3>NER (AnCora)</h3><p>For Named Entity Recognition (NER) we use annotations from the of the wellknown AnCora corpus. We extracted Named Entities from the original AnCora version, filtering out some unconventional ones, like book titles, and transcribed them into a standard CONLL-IOB format. The AnCora corpus has been released recently under CC-BY licence, and we published our derivative version under the same licence.</p></div><div class="datasetHolder" id="pos"><h3>POS (AnCora)</h3><p>For Part-of-Speech Tagging (POS) we use annotations from the of the wellknown AnCora corpus, projected on the Universal Dependencies treebank. The AnCora corpus has been released recently under CC-BY licence, and we published our derivative version under the same licence.</p></div><div class="datasetHolder" id="sts"><h3>STS-ca</h3><p>For Semantic Textual Similarity (STS), we create a new dataset from scratch: STS-ca. It contains more than 3,000 sentence pairs, annotated with their semantic similarity using a scale from 0 (no similarity at all) to 5 (semantic equivalence).
To develop STS-ca, we pre-selected potential sentence pairs from the CaText corpus by using different similarity measures (Jaccard, Doc2Vec and DistilBERT embedding cosine similarity). We did a final manual review to ensure that the selection represented superficial and deep similarities in subject matter and lexicon. Following the guidelines set in the SemEval challenges, we commissioned 4 native speaker annotators from 2 independent companies to assess the similarity of the sentence pairs on a scale between 0 (completely dissimilar) to 5 (completely equivalent), with other possible values, such as 3 (roughly equivalent, but some important information differs). Then, for each sentence pair, we computed the mean of the four annotations, and we discarded single annotations that deviate by more than 1 from the mean. After this filtering process, we used the mean of the remaining annotations as a final score. Finally, in order to assess the quality of the dataset, we measured the correlation of each annotator’s labels with the average of the rest of the annotators, and averaged all the individual correlations, resulting in a Pearson correlation of 0.739.</p></div><div class="datasetHolder" id="teca"><h3>TE-ca</h3><p>TeCa is a dataset of Textual Entailment in Catalan. It contains more than 20,000 pairs of sentences annotated with their relation label: Neutral, Inference or Contradiction. The source sentences of this dataset were extracted from the Catalan Textual Corpus and from VilaWeb newswire. We randomly chose 18,000 sentences from these sources, and filtered them by different criteria, such as length and stand-alone intelligibility. From the remaining text sentences, we commissioned 3 hypotheses (one for each entailment category) to be written by a team of annotators. We obtained more than 20,000 annotated sentence pairs, which are published under the CC-BY license. From 600 randomly selected samples, we cross-annotated for quality assurance and obtained an inter-annotator agreement of 83.57%.</p></div><div class="datasetHolder" id="tecla"><h3>TeCla</h3><p>TeCla (Textual Classification for Catalan) is a Catalan news corpus for thematic Text Classification tasks. It contains 153,265 articles classified under 30 different categories, albeit editorially-oriented ones rather than truly encyclopedic labels.
We crawled 219,586 articles from the Catalan News Agency (ACN) newswire archive, the latest from October 11th, 2020. We used the subsection category as a classification label, after excluding territorial labels and labels with less than 2,000 occurrences. With this criteria we compiled a total of 153,265 articles for this text classification dataset.</p></div><div class="datasetHolder" id="catalanqa"><h3>CatalanQA</h3><p>This dataset is an extractive Question Answering dataset that gathers the previous QA datasets ViquiQuAD and VilaQuAD. ViquiQuAD was developed using content from the Catalan Wikipedia (Viquipèdia). From a set of high quality, non-tranlsated articles in the Catalan Wikipedia, 597 were randomly chosen, and from them 5-8 sentence contexts were extracted. We commissioned the creation of between 1 and 5 questions for each context, following an adaptation of the guidelines from SQUAD 1.0. In total, 15,153 pairs of a question and an extracted fragment that contained the answer were created. Following the same guidelines as ViquiQuAD, we developed VilaQuAD, an extractive QA dataset from newswire. From a the online edition of the Catalan newspaper Vilaweb, 2,095 article headlines were randomnly selected. In total, 6,282 pairs of a question and an extracted fragment that contains the answer were created.</p></div><div class="datasetHolder" id="xquad"><h3>XQuAD-ca</h3><p>The Cross-lingual Question Answering Dataset is a multilingual benchmark for evaluating question-answering performance. The dataset consists of a subset of 240 paragraphs from the Wikipedia and 1,190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Rumanian was added later. We created a Catalan version using professional translators.</p></div><div class="datasetHolder"><h3></h3><p></p></div></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="centerNav"><div><img src="./secretaria-es.png"></div><div><img src="./gob-es.png"></div></div></div></nav></body></html>